{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Email Spam Filter\n",
    "Dataset:\n",
    "Goal:\n",
    "Attributes: \n",
    "    Email content\n",
    "    Label(spam/ham)\n",
    "Outline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# Import Scikit-learn helper functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Import Scikit-learn models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Import Scikit-learn metric functions\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(r\"fyp_dataset.csv\",encoding='latin1')\n",
    "#df.head()\n",
    "df = pd.read_csv(r\"trec07p1.csv\",encoding='latin1')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print counts of each label\n",
    "print(df['label'].value_counts())\n",
    "plt.pie(df['label'].value_counts(), labels=['ham','spam'],autopct=\"%0.2f\")\n",
    "plt.title('Counts of Each label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URL extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_ex(dfrow):\n",
    "    url = re.findall(\"(http.*)\\s\",dfrow['body'])\n",
    "    if len(url) > 0:\n",
    "        dfrow['url'] = url\n",
    "    else:\n",
    "        dfrow['url'] = None\n",
    "    return dfrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfurl = df.apply(url_ex,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StopWord Remove & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(body):\n",
    "    #Convert to lowercase \n",
    "    lower_body = body.lower() \n",
    "    #Remove punctuation\n",
    "    nonpunc = ''.join(char for char in lower_body if char not in string.punctuation)\n",
    "    #Remove extra spaces\n",
    "    nonspace = re.sub(r'\\s+', ' ', nonpunc)\n",
    "    #Remove stopwords\n",
    "    clean_body = ' '.join(word for word in nonspace.split() if word not in stopwords.words('english'))\n",
    "    #Tokenization\n",
    "    clean_body = nltk.word_tokenize(clean_body)\n",
    "    return clean_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the messages\n",
    "dfurl['clean_body'] = dfurl['body'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the null body\n",
    "def drop_null(processdf):\n",
    "    count = processdf['clean_body'].count()\n",
    "    drop_list = []\n",
    "    for i in range(count):\n",
    "        blen = len(list(processdf['clean_body'])[i])\n",
    "        if blen == 0:\n",
    "            drop_list.append(i)\n",
    "            print(i)\n",
    "        else:\n",
    "            pass\n",
    "    return processdf.drop(drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = drop_null(dfurl)\n",
    "df3.to_csv('tocken(preprocess).csv', index=False)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48788,) (12198,) (48788,) (12198,)\n"
     ]
    }
   ],
   "source": [
    "#Split Data to train &ã€€test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['body'],df['label'],test_size=0.2, random_state=1)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the training inputs -- Takes about 35 mins to complete\n",
    "#   There are two types of vectors: \n",
    "\n",
    "#     1. Count vectorizer\n",
    "print(\"- Training Count Vectorizer -\")\n",
    "countVec = CountVectorizer(tokenizer=tokenizer)\n",
    "train_count_x = countVec.fit_transform(x_train)\n",
    "\n",
    "#     2. Term Frequency-Inverse Document Frequency (TF-IDF)corpus\n",
    "print(\"- Training TF-IDF Vectorizer -\")\n",
    "tfidVec = TfidfVectorizer(tokenizer=tokenizer)\n",
    "train_tfidf_x = tfidVec.fit_transform(x_train)\n",
    "\n",
    "print(\"\\n### Vectorizing Complete ###\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the testing inputs -- Takes about 15 mins to complete\n",
    "\n",
    "print(\"- Count Vectorizer -\")\n",
    "test_count_x = countVec.transform(x_test)\n",
    "\n",
    "print(\"- Tfidf Vectorizer -\")\n",
    "test_tfidf_x = tfidVec.transform(x_test)\n",
    "\n",
    "print(\"\\n### Vectorizing Complete ###\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Evaluate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36145677, 0.25514595, 0.19135947, 0.10631081, 0.06378649,\n",
       "       0.31893244, 0.04252433, 0.06378649, 0.02126216, 0.29767028,\n",
       "       0.02126216, 0.02126216, 0.21262163, 0.06378649, 0.3401946 ,\n",
       "       0.14883514, 0.04252433, 0.02126216, 0.19135947, 0.42524326,\n",
       "       0.31893244, 0.21262163, 0.02126216, 0.02126216])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_X.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 24)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t15\n",
      "  (0, 1)\t12\n",
      "  (0, 19)\t20\n",
      "  (0, 20)\t15\n",
      "  (0, 0)\t17\n",
      "  (0, 9)\t14\n",
      "  (0, 14)\t16\n",
      "  (0, 6)\t2\n",
      "  (0, 15)\t7\n",
      "  (0, 12)\t10\n",
      "  (0, 23)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 21)\t10\n",
      "  (0, 16)\t2\n",
      "  (0, 18)\t9\n",
      "  (0, 7)\t3\n",
      "  (0, 22)\t1\n",
      "  (0, 13)\t3\n",
      "  (0, 2)\t9\n",
      "  (0, 3)\t5\n",
      "  (0, 4)\t3\n",
      "  (0, 17)\t1\n",
      "  (0, 10)\t1   (0, 10)\t0.021262162778128115\n",
      "  (0, 17)\t0.021262162778128115\n",
      "  (0, 4)\t0.06378648833438434\n",
      "  (0, 3)\t0.10631081389064058\n",
      "  (0, 2)\t0.19135946500315304\n",
      "  (0, 13)\t0.06378648833438434\n",
      "  (0, 22)\t0.021262162778128115\n",
      "  (0, 7)\t0.06378648833438434\n",
      "  (0, 18)\t0.19135946500315304\n",
      "  (0, 16)\t0.04252432555625623\n",
      "  (0, 21)\t0.21262162778128116\n",
      "  (0, 11)\t0.021262162778128115\n",
      "  (0, 8)\t0.021262162778128115\n",
      "  (0, 23)\t0.021262162778128115\n",
      "  (0, 12)\t0.21262162778128116\n",
      "  (0, 15)\t0.1488351394468968\n",
      "  (0, 6)\t0.04252432555625623\n",
      "  (0, 14)\t0.34019460445004984\n",
      "  (0, 9)\t0.2976702788937936\n",
      "  (0, 0)\t0.36145676722817793\n",
      "  (0, 20)\t0.3189324416719217\n",
      "  (0, 19)\t0.4252432555625623\n",
      "  (0, 1)\t0.25514595333753737\n",
      "  (0, 5)\t0.3189324416719217\n"
     ]
    }
   ],
   "source": [
    "print(count_X,tfidf_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv(r\"trec07p1.csv\",encoding='latin1')\n",
    "    df.info()\n",
    "    #Convert the body to a matrix of token counts trec07p1 (using function **process)\n",
    "    #extract feature\n",
    "    bow = CountVectorizer(analyzer=process).fit_transform(df['body'])\n",
    "    print(bow.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "60cbac361453a210bdf3909d936042b15e2c85753433183cd82a246aa446a9e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
